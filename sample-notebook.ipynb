{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Cloud AI Platform Notebook: Predicting Visitor Behaviour\n",
    "Predict if a visitor will add items to the cart using their browsing session data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and plot some data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Google Cloud datawharehouse [BigQuery](https://cloud.google.com/bigquery/docs/introduction) to query the data.  \n",
    "The BigQuery client library provides a cell magic ```%%bigquery``` which runs a SQL query and returns the results as a Pandas DataFrame.  \n",
    "Use the cell magic to query a sample of data and save the results in the ```train_df``` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery train_df\n",
    "SELECT \n",
    "  deviceCategory,\n",
    "  totalPageViews,\n",
    "  addedToCart\n",
    "FROM\n",
    "  `kaggleworkshops.google_analytics_sample_eu.train`\n",
    "WHERE date BETWEEN TIMESTAMP(\"2016-08-01\")\n",
    "  AND TIMESTAMP(\"2016-08-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first few rows of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the DataFrame details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data by ```deviceCategory``` and sort by ```totalPageViews```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('deviceCategory').sum().sort_values('totalPageViews', ascending=False).head(10).plot(kind='bar', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to train the model locally (from within the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to centrally store and share data across services.  \n",
    "Let's use [Google Cloud Storage](https://cloud.google.com/storage) which is the blob storage service from Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```gsutil``` is a command line tool for Google Cloud Storage.  \n",
    "In Google Cloud Storage, URI are in the format ```gs://bucket/folder/file``` .  \n",
    "Use ```gsutil mb gs://YourBucketName``` to create a Google Cloud Storage bucket.\n",
    "\n",
    "Your bucket name must be **globally** unique and must contain only lowercase letters, numbers, dashes, underscores, and dots.  \n",
    "**!!! Change the bucket name below with your own!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Google Cloud Storage directory to save logs and model\n",
    "BUCKET_NAME = 'ml-competition-mpe-team' # Create your own unique bucket name\n",
    "!gsutil mb -l EU gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```gsutil ls gs://YourBucketName``` to list your bucket and make sure it has been correctly created.  \n",
    "Expect no output if the bucket is correctly created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your trainer package structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create your trainer package structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer package structure\n",
    "!mkdir ./trainer\n",
    "!touch ./trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! Change the bucket name below with your own!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/task.py\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Declare constants\n",
    "BUCKET_NAME = 'ml-competition-mpe-team'  # change to YOUR bucket name\n",
    "JOB_DIR = f'gs://{BUCKET_NAME}/latest_model' \n",
    "CATEGORICAL_TYPES = {'deviceCategory': pd.api.types.CategoricalDtype(\n",
    "                                ['desktop', 'mobile', 'tablet']\n",
    "                            )}\n",
    "TARGET_COLUMN = 'addedToCart'\n",
    "QUERY = '''SELECT \n",
    "  deviceCategory,\n",
    "  totalPageViews,\n",
    "  addedToCart\n",
    "FROM\n",
    "  `kaggleworkshops.google_analytics_sample.train`\n",
    "WHERE date BETWEEN TIMESTAMP(\"2016-08-01\")\n",
    "  AND TIMESTAMP(\"2016-08-31\")'''\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Read the data from BigQuery\n",
    "client = bigquery.Client() \n",
    "query_job = client.query(QUERY)\n",
    "data_df = query_job.to_dataframe()  # you can read from other sources to pandas DataFrame\n",
    "print(f'First rows for the raw dataset: \\n{data_df.head()}')\n",
    "\n",
    "# Convert integer valued (numeric) columns to floating point\n",
    "numeric_columns = data_df.select_dtypes(['int64']).columns\n",
    "data_df[numeric_columns] = data_df[numeric_columns].astype('float32')\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "cat_columns = data_df.select_dtypes(['object']).columns\n",
    "data_df[cat_columns] = data_df[cat_columns].astype('category')\n",
    "data_df[cat_columns] = data_df[cat_columns].apply(lambda x: x.astype(\n",
    "        CATEGORICAL_TYPES[x.name]))\n",
    "data_df[cat_columns] = data_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "print(f'First rows for the transformed dataset: \\n{data_df.head()}')\n",
    "\n",
    "# Train/Val split\n",
    "train_df, val_df = train_test_split(data_df, train_size=0.8)\n",
    "train_target = train_df.pop(TARGET_COLUMN)\n",
    "val_target = val_df.pop(TARGET_COLUMN)\n",
    "num_train_examples = len(train_df)\n",
    "num_val_examples = len(val_df)\n",
    "\n",
    "# *Possible improvements*: add standartization for numeric values to range [-1; 1], categories to one-hot encoded\n",
    "\n",
    "# Creata tensorflow dataset object\n",
    "dataset_train = (tf.data.Dataset\n",
    "                 .from_tensor_slices((train_df.to_dict('list'), train_target))\n",
    "                 .shuffle(buffer_size=BATCH_SIZE*4)\n",
    "                 .repeat()\n",
    "                 .batch(BATCH_SIZE))\n",
    "                 \n",
    "dataset_val = (tf.data.Dataset\n",
    "                 .from_tensor_slices((val_df.to_dict('list'), val_target))\n",
    "                 .repeat()\n",
    "                 .batch(BATCH_SIZE))  # No shuffle\n",
    "\n",
    "print(f'One batch of the train data:\\n {next(iter(dataset_train))}')\n",
    "\n",
    "# Prepare named inputs for our model\n",
    "inputs = {key: tf.keras.layers.Input(shape=(), name=key) for key in train_df.keys()}\n",
    "x = tf.stack(list(inputs.values()), axis=-1)\n",
    "\n",
    "# Define model's architecture\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "# Build the model and compile it\n",
    "model_func = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "model_func.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "# Train the model\n",
    "history = model_func.fit(dataset_train, \n",
    "                          epochs=NUM_EPOCHS, \n",
    "                          steps_per_epoch=int(num_train_examples/BATCH_SIZE), \n",
    "                          validation_data=dataset_val, \n",
    "                          validation_steps=int(num_train_examples/BATCH_SIZE), \n",
    "                          verbose=1)\n",
    "\n",
    "model_func.save(f'{JOB_DIR}/export/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['scikit-learn', 'pandas']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My super training application package.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a local training task\n",
    "This is a good test before your will try to train the model in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many commands we are going to use accept a parameter for setting a region.  \n",
    "A region is a group of Google Cloud data centers used to run computing tasks.  \n",
    "To reduce latency, let's set a variable with a close by data center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interacting with Cloud AI Platform we are going to use the [gcloud](https://cloud.google.com/sdk/gcloud) command line tool.  \n",
    "Gcloud also provides [properties](https://cloud.google.com/sdk/docs/properties) used by other services.  \n",
    "Let's set the __ml_engine/local_python__ property so AI Platform knows which Python version to use for local training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
    "!gcloud config set ml_engine/local_python $(which python3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submitting a local training job to AI Platform you need to provide:\n",
    "* A directory to store the model and logs: here we are going to use our Google Cloud Storage bucket we created earlier\n",
    "* The path to your trainer package\n",
    "* The name of your trainer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a timestamped job name\n",
    "JOB_NAME = f\"training_job_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"; print(JOB_NAME)\n",
    "JOB_DIR = f'gs://{BUCKET_NAME}/{JOB_NAME}'; print(JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our local training job with the gcloud command ```gcloud ai-platform local train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the localtraining job\n",
    "! gcloud ai-platform local train \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path ./trainer \\\n",
    "  --module-name trainer.task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model has been saved to your Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model in the cloud.  \n",
    "This can help when you'll need more compute power, run your training for a long periods of time or try several trainings in parallel with hyperparameters search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training through AI Platform you need a few more parameters:\n",
    "* __Region__: the region used by AI Platform for training\n",
    "* __Runtime version__: the AI Platform version you want to use\n",
    "* __Python version__: the Python version used by your package\n",
    "* __Scale tier__: define which compute power will be used (GPU, TPU, number of machines, ...), more details in [this documentation](https://cloud.google.com/ai-platform/training/docs/machine-types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit a training job with ```gcloud ai-platform jobs submit training``` with a basic configuration (only 1 machine, no GPU, no TPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --package-path ./trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version=2.1 \\\n",
    "  --python-version=3.7 \\\n",
    "  --scale-tier basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job is a long running operation.  \n",
    "You can use ```gcloud ai-platform jobs describe``` to get the status of the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model has been saved to your Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform deployment\n",
    "\n",
    "Now that you have trained your model, it's time to make it available for serving predictions.  \n",
    "Google [AI Platform Prediction](https://cloud.google.com/ai-platform/prediction/docs) lets you do just that very easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use ```gsutil ls``` to list your model's file from the Cloud Storage Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_MODEL_DIR = f'gs://{BUCKET_NAME}/latest_model/export' \n",
    "!gsutil ls -lh $LATEST_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a **name** and a **version** for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'clf_add_to_cart' # Choose your own model name\n",
    "MODEL_VERSION = 'v1' # Make sure to increase version when deploying a new version of the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use ```gcloud ai-platform models create```to create a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object at AI Platform first\n",
    "! gcloud ai-platform models create $MODEL_NAME \\\n",
    "  --regions $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model available in AI Platform, let's create the first version of this model.  \n",
    "We need to point AI Platform to our model in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model version based on that SavedModel directory\n",
    "! gcloud ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --region global \\\n",
    "  --runtime-version 2.1 \\\n",
    "  --python-version 3.7 \\\n",
    "  --framework tensorflow \\\n",
    "  --origin $LATEST_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with AI Platform (from a csv file)\n",
    "At this point we have trained a model and made the model available for serving predictions thanks to AI Platform.  \n",
    "Let's get some predictions from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test data\n",
    "Let's grab some fresh data to generate predictions on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery test_df\n",
    "SELECT \n",
    "  CONCAT(fullVisitorId, CAST(visitStartTime as string)) as sessionId, \n",
    "  deviceCategory,\n",
    "  totalPageViews\n",
    "FROM\n",
    "  `kaggleworkshops.google_analytics_sample_eu.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby('deviceCategory').sum().sort_values('totalPageViews', ascending=False).head(10).plot(kind='bar', logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to avoid \"serving skew\"! \n",
    "# Preprocess test data the same way as we did for training\n",
    "CATEGORICAL_TYPES = {'deviceCategory': pd.api.types.CategoricalDtype(\n",
    "                                ['desktop', 'mobile', 'tablet']\n",
    "                    )}\n",
    "\n",
    "# Convert integer valued (numeric) columns to floating point\n",
    "numeric_columns = test_df.select_dtypes(['int64']).columns\n",
    "test_df[numeric_columns] = test_df[numeric_columns].astype('float32')\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "cat_columns = list(CATEGORICAL_TYPES.keys())\n",
    "test_df[cat_columns] = test_df[cat_columns].astype('category')\n",
    "test_df[cat_columns] = test_df[cat_columns].apply(lambda x: x.astype(\n",
    "        CATEGORICAL_TYPES[x.name]))\n",
    "test_df[cat_columns] = test_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "print(f'First rows for the transformed dataset: \\n{test_df.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare prediction input file\n",
    "The `gcloud` command-line tool accepts newline-delimited JSON for online\n",
    "prediction, and this particular Keras model expects a flat list of\n",
    "numbers for each input example.\n",
    "\n",
    "AI Platform requires a different format when you make online prediction requests to the REST API without using the `gcloud` tool. The way you structure\n",
    "your model may also change how you must format data for prediction. Learn more\n",
    "about [formatting data for online\n",
    "prediction](https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview#prediction_input_data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"instances\": [\n",
    "  {\"deviceCategory\": 1, \"totalPageViews\": 10},\n",
    "  {\"deviceCategory\": 2, \"totalPageViews\": 1}\n",
    "]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test first on a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the prediction input to a JSON file in the format accepted by AI Platform\n",
    "import json\n",
    "\n",
    "prediction_dict_sample = test_df.drop('sessionId', axis=1)[:5].to_dict('records')\n",
    "\n",
    "with open('prediction_input.json', 'w') as json_file:\n",
    "    json.dump({'instances': prediction_dict_sample}, json_file, indent=' ')\n",
    "\n",
    "! cat prediction_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions on few samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```gcloud ai-platform predict``` to generate predictions from your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --region global \\\n",
    "  --version $MODEL_VERSION \\\n",
    "  --json-request prediction_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online predictions on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get predictions for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper copied from the AI Platform console\n",
    "import googleapiclient.discovery\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the Cloud ML Engine Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide our Project ID to the online predictions service.  \n",
    "Project ID is a unique identifier for the Google Cloud environment you are currently using.  \n",
    "Let's use ```gcloud config get-value project``` to get this property from gcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !gcloud config get-value project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = PROJECT_ID.get_nlstr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_predictions = partial(\n",
    "    predict_json,\n",
    "    project=PROJECT_ID, \n",
    "    model=MODEL_NAME, \n",
    "    version=MODEL_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(test_df)//BATCH_SIZE; num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = []\n",
    "session_ids = test_df.pop('sessionId')\n",
    "\n",
    "for i in tqdm(range(num_batches+1), total=num_batches, position=0):\n",
    "    batch_df = test_df.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:]\n",
    "    pred = get_predictions(instances=batch_df.to_dict('records'))\n",
    "    pred = [p['dense_1'][0] for p in pred]\n",
    "    prediction_scores.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictions, let's export them as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(\n",
    "    {'sessionId': session_ids, \n",
    "     'addedToCart':prediction_scores}, \n",
    "    columns=['sessionId', 'addedToCart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all versions and all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import googleapiclient.discovery\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "\n",
    "project = !gcloud config get-value project\n",
    "project = project.get_nlstr()\n",
    "\n",
    "def get_models(project):\n",
    "    response = service.projects().models().list(\n",
    "        parent = 'projects/{}'.format(project)\n",
    "    ).execute()\n",
    "    \n",
    "    return response[\"models\"]\n",
    "\n",
    "def get_versions(model):\n",
    "    response = service.projects().models().versions().list(\n",
    "        parent=model\n",
    "    ).execute()\n",
    "    \n",
    "    return response[\"versions\"]\n",
    "\n",
    "def delete_version(version):\n",
    "    print(\"Deleting version: \", version[\"name\"])\n",
    "    \n",
    "    response = service.projects().models().versions().delete(\n",
    "        name=version[\"name\"]\n",
    "    ).execute()\n",
    "    \n",
    "    if \"error\" in response:\n",
    "        print(error)\n",
    "    \n",
    "    return response[\"name\"]\n",
    "\n",
    "def delete_model(model):\n",
    "    print(\"Deleting model: \", model[\"name\"])\n",
    "    \n",
    "    response = service.projects().models().delete(\n",
    "        name=model[\"name\"]\n",
    "    ).execute()\n",
    "    \n",
    "    if \"error\" in response:\n",
    "        print(error)\n",
    "\n",
    "def is_version_deleted(operation):\n",
    "    print(\"Checking status for operation: \", operation)\n",
    "    \n",
    "    response = service.projects().operations().get(\n",
    "        name=operation\n",
    "    ).execute()\n",
    "    \n",
    "    print(response)\n",
    "    if \"done\" in response:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "models = get_models(project)\n",
    "\n",
    "default_version_deletions = []\n",
    "for model in models:\n",
    "    print('Model: ', model[\"name\"])\n",
    "    versions = get_versions(model[\"name\"])\n",
    "    deletions_in_progress = []\n",
    "    for version in versions:\n",
    "        # Delete non default versions\n",
    "        if \"isDefault\" not in version:\n",
    "            versions.remove(version)\n",
    "            deletions_in_progress.append(delete_version(version))\n",
    "    while len(deletions_in_progress) > 0:\n",
    "        # Try again in 5s\n",
    "        print(\"Waiting 5s\")\n",
    "        time.sleep(5)\n",
    "        for deletion_in_progress in deletions_in_progress:\n",
    "            if is_version_deleted(deletion_in_progress):\n",
    "                print(\"Deletion completed: \", deletion_in_progress)\n",
    "                deletions_in_progress.remove(deletion_in_progress)\n",
    "    # When all default versions are deleted, remove the default version\n",
    "    default_version_deletions.append(delete_version(versions[0]))\n",
    "while len(default_version_deletions) > 0:\n",
    "    # Try again in 5s\n",
    "    print(\"Waiting 5s\")\n",
    "    time.sleep(5)\n",
    "    for default_version_deletion in default_version_deletions:\n",
    "        if is_version_deleted(default_version_deletion):\n",
    "            print(\"Deletion completed: \", default_version_deletion)\n",
    "            default_version_deletions.remove(default_version_deletion)\n",
    "# All versions deleted, now delete the model\n",
    "for model in models:\n",
    "    delete_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete your bucket\n",
    "!gsutil rm -r gs://{BUCKET_NAME}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
